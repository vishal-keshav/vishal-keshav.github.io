[{"authors":["admin"],"categories":null,"content":"Hi, I am a Machine Learning Engineer with 3+ years of experience and my work revolves around defining Machine Learning problems, proposing innovative solutions and building research prototypes. I have developed end-to-end machine learning solutions and platform that runs not on servers but on smart devices demanding absolute precision in terms of accuracy and execution efficiency.\nNews Update: An article titled \u0026ldquo;Prior belief and its relationship with regularization\u0026rdquo; is live!!!\n","date":1557878400,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1557878400,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"Hi, I am a Machine Learning Engineer with 3+ years of experience and my work revolves around defining Machine Learning problems, proposing innovative solutions and building research prototypes. I have developed end-to-end machine learning solutions and platform that runs not on servers but on smart devices demanding absolute precision in terms of accuracy and execution efficiency.\nNews Update: An article titled \u0026ldquo;Prior belief and its relationship with regularization\u0026rdquo; is live!!!","tags":null,"title":"Vishal Keshav","type":"authors"},{"authors":null,"categories":null,"content":" Flexibility This feature can be used for publishing content such as:\n Online courses Project or software documentation Tutorials  The courses folder may be renamed. For example, we can rename it to docs for software/project documentation or tutorials for creating an online course.\nDelete tutorials To remove these pages, delete the courses folder and see below to delete the associated menu link.\nUpdate site menu After renaming or deleting the courses folder, you may wish to update any [[main]] menu links to it by editing your menu configuration at config/_default/menus.toml.\nFor example, if you delete this folder, you can remove the following from your menu configuration:\n[[main]] name = \u0026quot;Courses\u0026quot; url = \u0026quot;courses/\u0026quot; weight = 50  Or, if you are creating a software documentation site, you can rename the courses folder to docs and update the associated Courses menu configuration to:\n[[main]] name = \u0026quot;Docs\u0026quot; url = \u0026quot;docs/\u0026quot; weight = 50  Update the docs menu If you use the docs layout, note that the name of the menu in the front matter should be in the form [menu.X] where X is the folder name. Hence, if you rename the courses/example/ folder, you should also rename the menu definitions in the front matter of files within courses/example/ from [menu.example] to [menu.\u0026lt;NewFolderName\u0026gt;].\n","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536451200,"objectID":"59c3ce8e202293146a8a934d37a4070b","permalink":"/courses/example/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/courses/example/","section":"courses","summary":"Learn how to use Academic's docs layout for publishing online courses, software documentation, and tutorials.","tags":null,"title":"Overview","type":"docs"},{"authors":null,"categories":null,"content":" In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 2 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"74533bae41439377bd30f645c4677a27","permalink":"/courses/example/example1/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example1/","section":"courses","summary":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim.","tags":null,"title":"Example Page 1","type":"docs"},{"authors":null,"categories":null,"content":" Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 4 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"1c2b5a11257c768c90d5050637d77d6a","permalink":"/courses/example/example2/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example2/","section":"courses","summary":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus.","tags":null,"title":"Example Page 2","type":"docs"},{"authors":["Vishal Keshav"],"categories":[],"content":" Introduction Regularization techniques are a widely explored topic in machine learning that is used for improving generalization accuracy or reducing the overfitting of the data in a machine learning model. Regularization strategies involve either imposing a hard (or explicit) constraint or a soft (or implicit) constraints on the parameters of the model. Ridge regularizer (often referred to as $L2$) and Lasso regularizer (often referred to as $L1$) are some of the explicit regularizers that impose a direct constraint on the parameter. For example, $L1$ constraints the parameters in a space such that most of the parameter values equal to zero. Implicit regularization approach imposes a soft constraint on the parameters by modifying the optimization function. A popular approach called dropout is an example of such implicit regularizer that tunes the values of the parameters by the means of optimization function.\nThe goal of this article is neither to list down the regularization techniques commonly used in machine learning nor is to describe when and how the regularizers are being used, but to take a step back and understand the importance of prior (an expression of our belief on the values of the parameters) that is embodied in any regularization approach. Towards this end, we will look into two fundamental approaches of parameter estimation methods namely \u0026ldquo;Maximum Likelihood Estimation (MLE)\u0026rdquo; and \u0026ldquo;Maximum a Posteriori (MAP)\u0026rdquo; and see how the later approach regularizes the model better by using prior knowledge about the estimate. We will start with the first approach, MLE.\nMLE Overfits Any meaningful data has some underlying statistical property. For any data we get, it is reasonable to assume that there exists a data generating distribution that has generated the subset of available data. Our objective is to infer the distribution by analyzing the hidden statistical property in the observed data.\nLet us assume that the available data is given by $D$ where $D$ = $(x_{1},x_{2},\u0026hellip;,x_{n})$ and each $x_{i}$ corresponds to $i^{th}$ data point and $x_{i}$ $\\in$ $\\mathbb{R}^{d}$. Furthermore, we assume that each data point is being generated independently by a common data generating distribution denoted by $P(X/\\theta)$ or in other words, each data point $x_{i}$ is sampled from a random variable $X_{i}$ $\\sim$ $P$.\nWe intend to estimate the true parameter $\\theta$ of the assumed data distribution. MLE approaches this problem by maximizing the likelihood function $P(data/\\theta)$, or more formally $$ \\begin{equation} \\theta_{MLE} = \\operatorname*{arg\\,max}_\\theta P(X_{1}=x_{1},X_{2}=x_{2},\u0026hellip;,X_{n}=x_{n}/\\theta) \\end{equation} $$\nSince it is assumed that the data distribution are identical and independent, by using the conditional independence rule (conditioned on $\\theta$) on the joint probability distribution (over $X_{i}$), we get $$ \\begin{equation} \\theta_{MLE} = \\operatorname*{arg\\,max}_\\theta \\prod_{i=1}^{n}P(X_{i}/\\theta) \\end{equation} $$\nTaking a concrete example of data distribution, we can derive the maximum likelihood estimate. Lets suppose that data follows a Gaussian distribution (which is a reasonable assumption for scalar data points), and the parameters we are interested in is the mean (denoted by $\\mu$) where we keep the variance (denoted by $\\sigma^{2}$) a constant, i.e. $X_{i}$ $\\sim$ $\\mathcal{N}(\\mu , \\sigma^{2})$. Here, the only parameter we want to estimate is the $\\theta = \\mu$.\nFollowing the Gaussian density function for the given data distribution and the Independent and identically distributed (IID) assumptions, from previous equation we get $$ \\begin{equation} \\theta_{MLE} = \\operatorname*{arg\\,max}_\\theta \\left ( \\frac{1}{\\sqrt{2\\pi\\sigma^{{2}}}} \\right ) exp\\left ( -\\frac{1}{2\\sigma^{2}}\\sum_{i=1}^{n}(x_{i} - \\mu)^2 \\right ) \\end{equation} $$\nSince the logarithm is a monotonically increasing function, we can maximize $\\log$ of the right side expression and equivalently get the parameter of interest.\n$$ \\begin{align} \\theta_{MLE} \u0026amp; = \\operatorname*{arg\\,max}_\\theta \\log P(data/\\theta) \\\\\n\u0026amp; = \\operatorname*{arg\\,max}_\\theta -\\frac{n}{2}\\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2}\\sum_{i=1}^{n}(x_{i} - \\mu) \\end{align} $$\nfetches us $\\mu$ = $\\frac{\\sum_{i=1}^{n}x_{i}}{n}$ which is the sample mean. Here we can see that if the dataset is small, the MLE estimate memorizes both the signal and the noise from the data even if we may have some idea of where the parameter should lie.\nAn apt representation of this is idea is shown in the figure below.\nWhen the number of parameter increases and the number of data point remains small, the MLE estimates a complicated function that more often tries to fit the noise present in the data and diverges from the actual data distribution. In the next section, we will see how the MAP estimate overcome this drawback by introducing something called \u0026ldquo;prior\u0026rdquo;.\nMAP Regularizes Our problem statement remains the same as that of MLE, but our assumptions about the underlying statistical properties of the data is a bit different. We now assume that we have a full knowledge about the joint distribution of data and the underlying data distribution\u0026rsquo;s parameter, as denoted by $P(X_{1}, X_{2},\u0026hellip;,X_{n},\\theta)$. Here, we note that the $\\theta$ is now a random variable. Although $\\theta$ is a random variable now, MAP estimates are the point estimates that evaluate the parameter that maximizes a certain condition, in this case it is the posterior distribution of $\\theta$ given the data. Formally,\n$$ \\begin{align} \\theta_{MAP} \u0026amp; = \\operatorname*{arg\\,max}_\\theta P(\\theta/X_{1},X_{2},\u0026hellip;,X_{n})\\\\\n\u0026amp; = \\operatorname*{arg\\,max}_\\theta \\frac{P(X_{1}, X_{2},\u0026hellip;,X_{n},\\theta)}{P(X_{1}, X_{2},\u0026hellip;,X_{n})}\\\\\n\u0026amp; = \\operatorname*{arg\\,max}_\\theta P(X_{1}, X_{2},\u0026hellip;,X_{n}/\\theta)*P(\\theta) \\end{align} $$\nusing proportionality and Bayes theorem. Taking log and using IID assumption on right-hand side of the expression, we get $$ \\begin{equation} \\theta_{MAP} = \\operatorname*{arg\\,max}_\\theta \\sum_{i=1}^{n}\\log P(X_{i} = x_{i}/\\theta) + \\log P(\\theta) \\end{equation} $$\nWe again take a concrete example by assuming a joint distribution of data and parameter. Alternatively, we assume that the conditional data distribution conditioned on parameter $\\mu$ follows $\\mathcal{N}(\\mu, \\sigma^{2})$ where $\\theta = \\mu$, $\\sigma$ is kept as a constant and the parameter $\\mu$ is distributed normally with mean $\\mu^{*}$ and a variance of 1 i.e. $\\mu$ = $\\theta$ $\\sim$ $\\mathcal{N}(\\mu^{*}, 1)$. Under these univariate Gaussian assumptions for data and parameter, we can maximize the logarithm of the posterior to estimate the point estimate for $\\theta$. $$ \\begin{align} \\theta_{MAP} = \\operatorname*{arg\\,max}_\\theta \u0026amp; \\left ( \\frac{1}{\\sqrt{2\\pi\\sigma^{{2}}}} \\right ) exp\\left ( -\\frac{1}{2\\sigma^{2}}\\sum_{i=1}^{n}(x_{i} - \\mu)^2 \\right ) + \\\\\\ \u0026amp; \\left ( \\frac{1}{\\sqrt{2\\pi}} \\right ) exp\\left ( -\\frac{1}{2}(\\mu - \\mu^{*})^2 \\right ) \\end{align} $$\nDifferentiating the right-hand side and equating it to zero, the expression evaluates to $$ \\begin{equation} \\frac{1}{\\sigma^{2}}\\left ( \\sum_{i=1}^{n}(x_{i}-n\\mu)^{2} \\right ) + (\\mu^{*} - \\mu) = 0 \\end{equation} $$ Rearranging the $\\mu$ on one side, we get $\\mu = \\frac{\\sum_{i=1}^{n}x_{i}}{\\sigma^2 + n} + \\frac{\\sigma^{2}}{\\sigma^{2}+n}\\mu^{*}$ Since, $\\overline{x} = \\frac{\\sum_{i=1}^{n}x_{i}}{n}$ is the sample mean, we can write the above equation as $$ \\begin{equation} \\theta_{MAP} = \\frac{n}{\\sigma^2 + n}\\overline{x} + \\frac{\\sigma^{2}}{\\sigma^{2}+n}\\mu^{*} \\end{equation} $$ From the above expression, we can see that the $\\theta_{MLE}$ is a convex combination of sample mean $\\overline{x}$ and the prior mean $\\mu^{*}$. The prior mean $\\mu^{*}$ reflects our prior belief about where the mean should be before we observe any data. When the number of data points are less, more weightage is given on the prior mean which helps in achieving regularization through reducing the data overfitting. In an extream case where $n \\rightarrow \\infty$, $\\frac{n}{\\sigma^2 + n} \\rightarrow 1$ and $\\frac{\\sigma^{2}}{\\sigma^{2}+n} \\rightarrow 0 $, the parameter estimation is dominated by sample mean (which will reasonably estimate the actual parameter of the data distribution). When the number of data points are sufficient, the parameter estimate is well guided by the observed data, otherwise we partially rely on our prior knowledge of what value a parameter should assume.\nRefer the figure below for an intuitive understanding. Summary In this article, we explored two fundamental parameter estimation algorithm namely Maximum Likelihood Estimation and Maximum a Posteriori. By taking concrete examples of the distribution (along with several reasonable assumptions), we argued that MAP estimates induce regularization through prior over the parameters.\nIn a traditional Bayesian setup, a prior is explicitly modeled, but in a deep learning setting, this may take several forms. Whatever be the case, prior knowledge induces better inductive bias and this can be achieved through designing the architecture, curating of the training data and the choosing a better optimization objective. A more direct approach of imposing the prior for regularizing a model can be seen in the process of distillation. A teacher network is trained without any prior data, and when it is trained, the knowledge (in terms of prior) is distilled in the student network. Formulating a better and more explicit approach to induce prior in deep learning systems is an active area of research which we will explore further in subsequent articles.\n","date":1557878400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557878400,"objectID":"5a3367d62d391224abafb9e8797a3c3e","permalink":"/post/regularization/","publishdate":"2019-05-15T00:00:00Z","relpermalink":"/post/regularization/","section":"post","summary":"In this article, we explore the relationship between regularization and the prior belief from baysian point of view.","tags":["Academic"],"title":"Priors and the relationship with regularization","type":"post"},{"authors":null,"categories":null,"content":"","date":1548527400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1548527400,"objectID":"c8dca5a63ec9af5c77f1fd96a6966c38","permalink":"/project/fast-prototype/","publishdate":"2019-01-27T00:00:00+05:30","relpermalink":"/project/fast-prototype/","section":"project","summary":"A framework, built on top of tensorflow, to quickly prototype research ideas.","tags":["Machine Learning"],"title":"Fast Prototype","type":"project"},{"authors":null,"categories":null,"content":"","date":1537986600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1537986600,"objectID":"799d73ce11f7b629b2167830f6160f65","permalink":"/project/interpreter/","publishdate":"2018-09-27T00:00:00+05:30","relpermalink":"/project/interpreter/","section":"project","summary":"A simplistic interpreter developed in Prolog-programming language.","tags":["CodeOn"],"title":"Interpreter","type":"project"},{"authors":null,"categories":null,"content":"","date":1537986600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1537986600,"objectID":"77367d5595d0ad03aa84370832243e09","permalink":"/project/parallel-matrix-multiplication/","publishdate":"2018-09-27T00:00:00+05:30","relpermalink":"/project/parallel-matrix-multiplication/","section":"project","summary":"Optimizing parallelized matrix multiplication by 1300 times (as compared to naive algorithm).","tags":["CodeOn"],"title":"Matrix-matrix multiplication","type":"project"},{"authors":null,"categories":null,"content":"","date":1537986600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1537986600,"objectID":"910d8a7458565b347f8b58fb3a61ee0f","permalink":"/project/py-game/","publishdate":"2018-09-27T00:00:00+05:30","relpermalink":"/project/py-game/","section":"project","summary":"A game developed in python, played by an agent, based on Q-learning algorithm","tags":["Machine Learning"],"title":"Py-Game","type":"project"},{"authors":null,"categories":null,"content":"","date":1537986600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1537986600,"objectID":"e9676b333a67c430be5edc24a0819058","permalink":"/project/tiny-ml-framework/","publishdate":"2018-09-27T00:00:00+05:30","relpermalink":"/project/tiny-ml-framework/","section":"project","summary":"A completely vectorized, fast ML-framework withing 700 lines of code","tags":["Machine Learning"],"title":"Tiny-ml-framework","type":"project"},{"authors":null,"categories":null,"content":"","date":1537986600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1537986600,"objectID":"f0e3105cffd36c12ace067e766bfc1fd","permalink":"/project/uva/","publishdate":"2018-09-27T00:00:00+05:30","relpermalink":"/project/uva/","section":"project","summary":"Solutions for 300 mind-boggling algorithm and data-structure problems from UVa judging site.","tags":["CodeOn"],"title":"UVa","type":"project"},{"authors":["Vishal Keshav"],"categories":[],"content":" More is Less: A More Complicated Network with Less Inference Complexity Problem solved: With each convolution layer, reduce the computation of convolution that results in zero activation.   Hints about zero features that may lead to zero activation is given by a parallel light-weight convolution (either spatial or depth wise) activation. LCCL (low cost collaborative layer) are parallel low cost convolutions providing zero outcome hints to main convolution. To increase the sparsity, methods such as using RelU activation and Batch Normalization is explored. Mentions of GEMM and GEMV for theoretical and practical acceleration improvements.  \u0026gt;LINK\nLearning Efficient Convolutional Networks through Network Slimming Problem solved: Reducing the input channels that have near zero activation, by enforcing L1 norm to be zero along with regularization effect. Lesser the input channels, faster the inference.   Input channel are pruned during the training by enforcing L1 norm to become zero. L1 regularization on scaling factor of batch normalization layers per channel does not require any architectural changes. Zero L1 enforcement per channel is achieved by addition of L1 norm of batch in the total cost function which then is needed to be minimized. Drop in the prediction accuracy can be restored by retraining with multi-pass scheme.  \u0026gt;LINK\nPruning Filters For Efficient Convnets Problem solved: Reduction in number of filters with low L1 norm. These filters are assumed to have insignificant contribution to output channels. With reduced filters, both complexity of computation of convolution at current layer and next layer decreases.   Filters in a given layer of a trained model are compared against each other based on L1 norm, and removed proportionally. Removal of filters is done in a single shot for whole network, after which training can be done to restore the accuracy Pruning filters across multiple layer can be done in either independent or greedy way. Sensitivity of pruning filters in a given layer is measured by the rate at which accuracy decreases with respect to number of pruned filter. Highly sensitive layers are left out for pruning.  \u0026gt;LINK\nData-Driven Sparse Structure Selection for Deep Neural Networks Problem solved: A systematic approach to prune structures is a deep network including filters or neurons, resnet blocks and grouped convolution (cardinality) is presented. Additional parameters in cost function is enforced to zero in order to realize structure pruning.   A general end-to-end framework for pruning structures at once without the need for retraining. Scaling factors is not needed to be necessarily associated to network weights (so does the gradients), these are generalized parameters and gradients are computed in additions to weight gradients Accelerated Proximal Gradient is adopted for scaling factor training  \u0026gt;LINK\nPruning Convolutional Neural Networks For Resource Efficient Inference Problem solved: Evaluates/proposes the greedy criterion for pruning feature maps with an objective of minimizing the cost difference before and after pruning along with L1 norm constraints on pruned weights. With FLOPs criteria, it achieves resource efficient inference.   It formalizes the gated pruning (i.e. if this feature channel be pruned or not) based on greedy criteria such as minimized weight, mean, deviation of activation, information gain, first order taylor expansion etc. System has two steps, pruning in which based on criteria, the importance of neurons is evaluated and pruned Training in which it trains the network with constrained cost object in consideration.  \u0026gt;LINK\nCondenseNet: An Efficient DenseNet using Learned Group Convolutions Problem solved: Improves the dense-net by introducing the concept of learned group convolution. Condensing and optimization (both being the part of training phase) helps prune filters per logical groups which gets re-arranged by indexing to formulate normal group convolution. Better design strategy for dense-net is also proposed.   Exponential growth rate of input channel is compensated by using depth wise separable convolution in case of dense net. Condense net improvises this by group convolution that learns the grouping. Method used allows groups to automatically select appropriate channels or not use some at all. Since this architecture follows dense-net, non-used channels will be useful in another layer. Condense stage involves pruning per group by sparsity regularizer and optimization stage (second half of training stage) involves re-arranging and indexing of input filters for group convolution.  \u0026gt;LINK\nDelugeNets: Deep Networks with Efficient and Flexible Cross-layer Information Inflows Problem solved: Effectively propagating cross layers parameters as compared to residual blocks but at the same time being more efficient that dense-nets. Through cross-layer depthwise convolution the efficient information flow is possible whilst providing more flexibility than resnets.   Per block, composite network is used (bottle neck design) having same map dimentions. In one block, inputs from each preceding layers are convolved by point wise convolution channel wise. In other words, for a given channel c, all the filter maps at channel c from preceding layers are segregated and convolved depthwise to get filter map of channel c in the input layer. Same process to all channels. For block transition (where dimention changes), 3X3 strided filters are applied to match the input dimentions. Whole idea is to have cross layer separable convolution to reduce computation complexity.  \u0026gt;LINK\nDeep Pyramidal Residual Networks Problem solved: Through empirical studies, it was found that increase in feature map dimention at residual blocks significantly increase the resnet burden at those layers. Thus, a gradual increase in feature map depth is introduced which increase accuracy.   Pyramidal shape is introduced which gradually increase the filters at every layer. Skip connections are done through zero padding instead of 1X1 convolution. Several pyramidal designs are explored which increase the filters as a function of layer index.  \u0026gt;LINK\nDeep Expander Networks: Efficient Deep Networks from Graph Theory Problem solved: Solves high connectivity (for good representational power by efficient information flow through network) but remaining under a given level of sparsity. This reduces the inference time with similar accuracy levels.   Explores graph theory (expanders graphs), in which layer connections are modelled as several bipartite graphs connected after each other. In each bipartite connections, limits the number of edges but still having all connections globally. For structured sparsity, random expanders and explcit expanders are proposed. Explicit expanders are formulated under XOR operation of some group generators and output vertex set.  \u0026gt;LINK\nAggregated Residual Transformations for Deep Neural Networks Problem solved: Improves the representational power of network to reduce the number of parameters used. Proposed cardinality as a hyper-parameter along with width and depth of the network which is easy to manage. Aggregation of transformed input with addition to produce output is proposed. Design space for mentioned network has lesser parameters (which is cardinality). Increasing cardinality is more effective than increasing depth or width to gain accuracy.   Homogeneous input transform is proposed for implementation which is followed by aggregation or addition (bottleneck architecture). Conceptual difference as compared to Inception-resnet lies in topology followed by each transformation paths. Its the same in this proposal. Conceptual difference as compared to group convolution lies in the fact that grouping can be done only upto one layer, whereas proposed system goes beyond one layer of transformation path.  \u0026gt;LINK\nDeepRebirth: Accelerating Deep Neural Network Execution on Mobile Devices Problem solved: Inference acceleration by streamline slimming (combining across layer depth) and branch slimming (combining along network width) of tensors and non-tensors. Sparsity of tensors are exploited with less relevant high utilization non-tensors through combination and retraining.   Observation about excessive runtimes of non-tensor units are made and possibility of combining those with tensor nearby tensor units is explored.  \u0026gt;LINK\nAccelerating Convolutional Neural Networks for Continuous Mobile Vision via Cache Reuse Problem solved: Faster inference in continuous image data stream in mobile device is proposed by considering the previous layer image similarity with the current input. The idea has been generalized to reusing the blocks of initial convolution computations. Image similarity is measure block wise such that similar blocks are invariant of translation (diamond search).   For computation overhead, image blocks are compared for similarity. For cache erosion (more relevant in deeper layers) where spatial location of data starts making less sense and cache reuse cannot be determined, reuse is restricted to initial layers. Similar to input raw image, convolution output can be treated as input for next layer are cache reuse by spatially comparing the similarity is possible.  \u0026gt;LINK\nBranchyNet: Fast Inference via Early Exiting from Deep Neural Networks Problem solved: Proposes the usage of branches in deeper network for early stopping and as a way to regularize network. Conventional joint optimization based training is used.   Branches helps in faster inferences where convolution of other branches are dropped. As a bi-product of this architecture, regularization and mitigation of vanishing gradient is achieved. Design space for putting branches at different entry points of main branch is explored. Better caching efficiency on CPU is shown.  \u0026gt;LINK\nCrescendo Net: A Simple Deep Convoltional Neural Network with Ensemble Behavior Problem solved: Basic building block called crescendo blocks are proposed wherein multiple convolution parallel layers with incremental depth enable whole network to act as ensembled network. As a results representational strength increases without using residuals.   The different depths of parallel paths lead to different receptive fields and therefore generate features in different abstract levels. Design space for crescendo blocks are explored for less hyper-parameter tuning. Memory efficient training is proposed where other parallel paths are frozen when training for one path is going on. Like fractals-nets, drop-connects is used (dropping paths) along with dropouts.  \u0026gt;LINK\nBlockDrop: Dynamic Inference Paths in Residual Networks Problem solved: Upon observation that human visual system spend less time on simple object and more on complex lead to a dynamic inferencing system that upon context (complexity of input), drops several convolution operation in between (can be modelled as dropping residual blocks). Policy for dropping residula blocks comes from policy network trained for maximizing accuracy while using minimum inference blocks(reward is formulated in such a way in reinforcement learning). Lesser computation on contextual inputs leads to reduction in inferencing time.   Pretrained resnets are jointly trained with policy network that has to output binary vector representing if blocks needs to be dropped or not based on difficulty of input image. Such policy network implicitly learns the input complexity representation. Drop or not per layer is modelled as K dimentional bernoulli. To train the policy network, expected reward is maximized by expected gradient training procedure. Initial steps use curriculum learning followed by joint tuning of policy network and resnet.  \u0026gt;LINK\nThiNet: A Filter Level Pruning Method for Deep Neural Network Compression Problem solved: Filters of a given layer is pruned such that output of next to next layer is not being affected. This method differs in the regard that it does not consider immediate layer's channel activation in its optimization problem instead relies of having a network with maximum representation capacity. Doing so leads to smaller network without accuracy loss.   After pruning, fine tuning is done to regain the accuracy. Method is data driven as training example is used to determine the importance of a filter based on changes in next to next layer\u0026rsquo;s output(sampled for different pixel, spatial location). Under optimization problem for next channel\u0026rsquo;s representaion strength, predefine compression rate is encoded which determines how many channels and hence how many previous layer filters has to be pruned.  \u0026gt;LINK\nEraseReLU: A Simple Way to Ease the Training of Deep Convolution Neural Networks Problem solved: Eased the training along with accuracy improvements along lesser amount of computations by observing that network become too non-linearized by stacking up non-linear units. It proposes to proportionally remove relu units from each block of network module.   Going from sigmoid to relu helped a portion of neurons to get non-linearized instead of individual neurons per layer. Still, for such layers, application of relu did not linearize negative units. Removing relu in proportion of number of layers helped to get all neurons linearize for several layers. Removing relu from several spots has lead to increase in representational power of networks with less complexity.  \u0026gt;LINK\nSEP-Nets: Small and Effective Pattern Networks Problem Solved: Observation that binarizing only 3X3 convolution(spatial feature extraction) and not 1X1 convolution (feature transformation) may lead to model compression with similar accuracy but a lower computation. Proposes Patter Residual Block on same concept, from which SEP-Nets module is constructed.   At an equivalent accuracy of MobileNets, model size has been compressed. Binarization can be done from initiation or train-binarize-tune method can be adopted. Binarization of non-transformation convolution helps faster computation at inference time. Instead of concatenation, addition operation is used where 1X1 works as inter residual connection.  \u0026gt;LINK\nLocal Binary Convolutional Neural Networks Problem solved: By employing the techniques of local binary convolution (predefined filters) instead of learnable weights, paper proposes to reduce the number of parameters required to convolve. Predefined convolution followed by non-linear activation and the followed by 1X1 learnable convolution composes LBCNN modules.   LBC anchor weights can be stochastically generated with required sparsity. Difference map from LBC is produced by similar convolution but with pre-defined weights. Variable pivot and ordering is hence defined by that. Bitmap from difference map is produced by using non-linearity such as sigmoid. To compose the feature map, normal 1X1 convolution is used.  \u0026gt;LINK\nTowards Accurate Binary Convolutional Neural Network Problem Solved: Unlike other binarization technique which straight forward binarizes weights and activation maps, this paper proposes to approximate the full precision by multiple binary filters or activations. With same accuracy, this technique has shown to be computationally efficient.   Using straight thorough estimator, binary filters can be trained. Further, binary filters(calculated using full precision weight mean and variance) may contain sift operator that can be learned. Binary filter coefficients are computed at every train forward iteration by solving linear regression over actual weights and approximated weights. Whole weight binarization and channel level binarization are suggested methods. Activation in the similar way is binarized using multiple binary activation maps, in order to accelerated bitwise computation on FPGA.  \u0026gt;LINK\nclcNet: Improving the Efficiency of Convolutional Neural Network using Channel Local Convolutions Problem Solved: This paper proposes a structured way of composing group convolution such that full channel receptive field in a given block is 100 percent. This introduces the idea of more generalized channel local convolution and acyclic graph called channel dependency graph that connects output channel to input channel in a convolution block and measure channel receptive field of that convolution block.   Observation such as having full channel receptive field in a convolution block is required to have efficient information flow for better representational power in a CNN. Interlaced group convolution, one of the two building block for cnc net is developed which with group convolution completes a convolution block. Determination of block parameters such as number of groups in IGC and GC is done through minimizing a developed cost function per convolution block remaining under the full channel receptive field constraints.  \u0026gt;LINK\nThe Enhanced Hybrid MobileNet Problem solved: With some heuristics, paper managed to improve the baseline mobilenet accuracy. Added a third parameter depth and played around with fractional pooling.   Have formulated the computation and parameters ratio with newly added depth parameter. With heuristics approach, improved the baseline accuracy while maintaining the parameter and computation ratio same.  \u0026gt;LINK\nDropMax: Adaptive Stochastic Softmax Problem Solved: Paper proposes to drop exponential terms from logits randomly so that network becomes an ensemble of exponential number of networks each trained on a different subset of problem. Logit dropout probabilities are learned during training steps.   Each training step is considered to be ensemble of exponential models. Computationally intractable probability calculation during inference is approximated with monte-carlo approximations. Idea is to drop classes which are not being confused much so that sub-problem being solved is smaller.  \u0026gt;LINK Improving CNN Performance with Min-Max Objective Problem solved: The activation at each layer is visualized to be an object manifold (or a space whose shape inherently represent something) where each dimension corresponds to objective classes to be learned by the network. A cost is proposed that directly considers activation map in optimization target.   Optimization tends to minimize compactness of each object manifold(feature variation) and maximize margin between different manifolds. Applying cost to arbitrary layer takes care of all the previous layers in network, due to backpropagation. To make gradient computation tractable, kernel trick is used.  \u0026gt;LINK\nDilated Residual Networks Problem solved: Empirical studies on the effect of output and receptive field leads to the conclusion that use of dilated convolution filters in the later layer in state-of-art networks increase accuracy.   Gridding artifacts due to dilation has been removed by removing max pooling and adding additional layers with decreasing dilation rates and removing residuals in those layers. Architecture for dilated residual network (DRN) is presented based on above consideration and it is shown that increase in accuracy has higher as compared to accuracy that would have resulted by increase in parameter.  \u0026gt;LINK\nInterleaved Group Convolutions for Deep Neural Networks Problem solved: Using two group convolution, an interleaved grouped convolution block is created which proves to be wider that normal convolution. Wider network helped network represent more and had higher accuracy.   Permutations in primary group convolution and secondary convolution each makes block equivalent to normal convolution. Interleaving in secondary group is done such that each group has one filter map from every primary group. After applying pointwise convolution on secondary group, channels are permuted back for input to next interleaved group convolution block. Optimal grouping factor has been derived to maximize width of the block.  \u0026gt;LINK\nIncomplete Dot Products for Dynamic Computation Scaling in Neural Network Inference Problem solved: A tunable is provided to adjust how many filters to be used during inference dynamically based of computation requirements. Filters are arranged from highly important to least important (similar to singular values in SVD). Profiles is presented by a set of coefficients (hinting how may filters to be used in each layer)   Incomplete dot product just does the dot products of filters and input channels as required by the profile. Arrangement of convolution channels and filter from high importance to low importance is possible because of the training procedure followed. All coefficient to one is equivalent to complete dot product.  \u0026gt;LINK\nSwGridNet: A Deep Convolutional Neural Network based on Grid Topology for Image Classification Problem solved: Proposed a grid like structure where output to a unit (convolution) is fed by splitting the input from previous block and through previous dimensionally neighboring unit. Such construction enables architecture to act like ensemble of many models as suggested by other simple architecture such as res-net.   Splitting is followed by convolution at grid level which is followed by concatenation from all the unit and input from previous block. Convolution units in a grid can be constructed by varying depth path(which is output feed to deeper neighbor) Number of output channels from each unit can be different from its neighboring units but total input will be equal to the sum of all outputs from neighboring units.  \u0026gt;LINK ShaResNet: reducing residual network parameter number by sharingweights Problem solved: Upon observation that in a group of residual blocks(same stage) with same dimensionality (where no resolution reduction is done), spatial convolution can be shared among those residual blocks. By using a shared filters among all the pointwise convolution (or non spatial convolution) on a given stage among res blocks, computation can be reduced with accuracy loss.   Network is trained normally, end-to-end Gradients are back propagated as usual to pointwise convolution and accumulated for shared weight per stage  \u0026gt;LINK\nConvolutional Networks with Adaptive Computation Graphs Problem solved: Paper proposes a method to adaptively switch off block computation (in res-net setting) and allow flow of data to net layers. This adaptive switching is based on input or context passed as an input. By constraining on number of times a layer should be switched off(enforcement during training step), gating mechanism adapts itself to switch off gates for certain inputs reducing computation complexity. Network is also robust to adversarial attacks.   Unlike highway network which is based on soft thresholding, this is hard thresholding. It focuses on utilizing important parts of computation block which have been made efficient to handle certain input. At training step, cost function constrain on number of times a layer should be executed. Soft decision of gating is used during training.  \u0026gt;LINK\nSkipNet: Learning Dynamic Routing in Convolutional Networks Problem solved: It is observed that a complex example needs few extra layers of computation to get correctly predicted. So for non-complex inputs, some layers can be skipped. Skipping a computation block is done through passing previous layer input to a gate(composed of RNN, shared by each layer) component. Using softmax, gate determines if layer be skipped or not.   Hybrid RL training procedure where supervised training step with softmax gating is used for backpropagation to work. RL is used to minimize the computation reward (less computation is rewarding) along with higher accuracy. LSTM RNN is used and shared among layers to reduce gating overhead.  \u0026gt;LINK Papers from 2016 EIE: Efficient Inference Engine on Compressed Deep Neural Network Problem solved: To fit whole network model in cache so that DRAM access is minimized, models are Compressed by involving techniques such as pruning and weight sharing. Once model are in cached, computation requires indirect access to weights(exploit static sparsity) and activations (exploit dynamic sparsity) which costs inefficiency. Thus compressed models are expanded. EIE develops a hardware design that directly works on compressed model using CSC format for inference.   CSC(compressed sparse column) format employees two vectors. First encodes shared weights and second encodes relative distance in cache memory. This design effectively works on fully connected layers. Hardware design includes description of all logical units and queuing/scheduling algorithms.  \u0026gt;LINK\nDeep Networks with Stochastic Depth Problem solved: Reduction is training time while improving the generalization accuracy by randomly shortcutting the inputs to next to next layer using identity function (as used in resnet)   Using bernoulli random variable, dropping the layer reduce the train time. At test time, layers are preserved. So, while training, networks are shallower and at inference, they are deeper. Increase in accuracy (as compared to similar static resnet design) is attributed to ensembling nature of many resnets.  \u0026gt;LINK\nPerforatedCNNs: Acceleration through Elimination of Redundant Convolutions Problem solved: Reduction in convolution computation by extrapolating some elements by nearby values. A perforated mask is created that describes what part needs to be have convolved value and which parts can be extrapolated by nearby value.   Perforation masks are created statically or upon the batch observations through out training. Static masks such as uniformly randomized, grid and pooling structured mask is proposed. Based on training data (statistically), impact mask is proposed that removes least important positions such that extrapolation effects on accuracy for those positions can be minimized. At each iteration, perforation rate at each layer is chosen such that impact on the loss is minimized. This is greedy approach.  \u0026gt;LINK\nPapers from 2015 and older Compressing Neural Networks with the Hashing Trick Problem solved: To reduce the model size, weight sharing techniques is proposed where instead of storing actual weight, hash key is stored and at inference time, hash value is restored from weight array.   Hash trick method is used.  \u0026gt;LINK\nTiled convolutional neural networks Problem solved: An approach is presented where a weight is untied or unshared uptil certain distance of spatial location. This can effectively produce invariance to rotation and scaling   Untying of weights are contrasted with tied weight where same set of parameter extracts features for a pixel location. An untied weight is equivalent to freely choose a different weight for a neighboring pixel. Using a different weight per patch remains valid for certain distance until receptive field for certain feature remains relevant. Unsupervised pre-training is used to initialize the weights before supervised backprop.  \u0026gt;LINK\n","date":1537401600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1515801600,"objectID":"cfd3c4a3bfc915d289c4e0bc1e2407b0","permalink":"/post/review-cv/","publishdate":"2018-09-20T00:00:00Z","relpermalink":"/post/review-cv/","section":"post","summary":"Computer vision has been dominated by deep learning architectures with convolution layers. Although, the accuracy of classification tasks has been improved, improvement in other dimentions such as reduction in floating point operations, reduction in parameters etc. has been widely explored. In this review, we brief on research happening in this space by presenting the recent architectural innovation for computer vision.","tags":["Academic"],"title":"A review of Computer Vision Architecture","type":"post"},{"authors":["Vishal Keshav"],"categories":null,"content":"","date":1535740200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1535740200,"objectID":"325eeea0dd16fe578f8bfb473114b11f","permalink":"/publication/illumination-estimation/","publishdate":"2018-09-01T00:00:00+05:30","relpermalink":"/publication/illumination-estimation/","section":"publication","summary":"This paper presents a novel design methodology for architecting a light-weight and faster DNN architecture for vision applications. The effectiveness of the architecture is demonstrated on Color-Constancy use case an inherent block in camera and imaging pipelines. Specifically, we present a multi-branch architecture that disassembles the contextual features and color properties from an image, and later combines them to predict a global property (e.g. Global Illumination). We also propose an implicit regularization technique by designing cross-branch regularization block that enables the network to retain high generalization accuracy. With a conservative use of best computational operators, the proposed architecture achieves state-of-the-art accuracy with 30X lesser model parameters and 70X faster inference time for color constancy. It is also shown that the proposed architecture is generic and achieves similar efficiency in other vision applications such as Low-Light photography.","tags":[],"title":"Decoupling Semantic Context and Color Correlation","type":"publication"},{"authors":["Vishal Keshav"],"categories":null,"content":"","date":1535740200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1535740200,"objectID":"5005baa07c273bfd7c78ddb2952f87db","permalink":"/publication/optimizating-wireless-serving/","publishdate":"2018-09-01T00:00:00+05:30","relpermalink":"/publication/optimizating-wireless-serving/","section":"publication","summary":"The main aim of the project is to study and apply Queueing Theory in modelling the wireless network node and create a power consumption function along with user specified constraints. Power consumption function is minimized satisfying the constraint using a numerical optimization technique. Optimised parameters are then tested on performance metrics developed.","tags":[],"title":"Optimisation of power consumption in wireless network","type":"publication"},{"authors":null,"categories":null,"content":"Machine learning has become ubiquitous and is now realized in almost every application. This has burdened the machine learning model developers to make necessary modification in the algorithm to select best available architecture such that performance and power needs can be satisfied post-deployment of the application to smart devices such as smartphones and wearables.\nFor most of the part, a developer tunes the model by changing several hyper-parameters. However, any choice of such hyper-parameters comes either by experience and deep insight, or certain guidelines as provided by the architecture developers (based on ablation studies).\nIn this project, we propose to develop a meta-learning algorithm that takes a specified model architecture and tunes it to match the performance requirement on a particular smart-device. More specifically, we propose to build a policy that iteratively removes weights, channels, and layers in a vision-based model to improve upon execution efficiency while maintaining the prediction accuracy. We show that once the policy is constructed on a particular platform, the domain knowledge can be transferred to construct a policy for a different platform.\n","date":1534636800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1534636800,"objectID":"41ff7bb1d020558b05d63fe3ff6dc7dd","permalink":"/project/xcelerator/","publishdate":"2018-08-19T00:00:00Z","relpermalink":"/project/xcelerator/","section":"project","summary":"Experimenting with RL algorithm for neural architecture search","tags":["Machine-Learning"],"title":"Xcelerator: Neural Architecture Search","type":"project"}]