<!DOCTYPE html>
<html lang="en-us">

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academic 4.8.0">

  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Vishal Keshav">

  
  
  
    
  
  <meta name="description" content="In this article, we explore the relationship between regularization and the prior belief from baysian point of view.">

  
  <link rel="alternate" hreflang="en-us" href="/post/regularization/">

  


  
  
  
  <meta name="theme-color" content="#2962ff">
  

  
  
  
  <script src="/js/mathjax-config.js"></script>
  

  
  
  
  
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.0-1/css/all.min.css" integrity="sha256-4w9DunooKSr3MFXHXWyFER38WmPdm361bQS/2KUWZbU=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css" integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/github.min.css" crossorigin="anonymous" title="hl-light">
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark" disabled>
        
      
    

    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.5.1/leaflet.css" integrity="sha256-SHMGCYmST46SoyGgo4YR/9AlK1vf3ff84Aq9yK4hdqM=" crossorigin="anonymous">
    

    

    
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/lazysizes/5.1.2/lazysizes.min.js" integrity="sha256-Md1qLToewPeKjfAHU1zyPwOutccPAm5tahnaw7Osw0A=" crossorigin="anonymous" async></script>
      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" integrity="" crossorigin="anonymous" async></script>
      
    
      

      
      

      
    

  

  
  
  
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Montserrat:400,700%7CRoboto:400,400italic,700%7CRoboto+Mono&display=swap">
  

  
  
  
  
  <link rel="stylesheet" href="/css/academic.css">

  





<script async src="https://www.googletagmanager.com/gtag/js?id=UA-131439197-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];

  function gtag() {
      dataLayer.push(arguments);
  }

  function trackOutboundLink(url, target) {
    gtag('event', 'click', {
         'event_category': 'outbound',
         'event_label': url,
         'transport_type': 'beacon',
         'event_callback': function () {
           if (target !== '_blank') {
             document.location = url;
           }
         }
    });
    console.debug("Outbound link clicked: " + url);
  }

  function onClickCallback(event) {
    if ((event.target.tagName !== 'A') || (event.target.host === window.location.host)) {
      return;
    }
    trackOutboundLink(event.target, event.target.getAttribute('target'));  
  }

  gtag('js', new Date());
  gtag('config', 'UA-131439197-1', {});

  
  document.addEventListener('click', onClickCallback, false);
</script>


  


  

  <link rel="manifest" href="/index.webmanifest">
  <link rel="icon" type="image/png" href="/images/icon_hue165b90e315bd195bed0e7c1c2e09531_14682_32x32_fill_lanczos_center_2.png">
  <link rel="apple-touch-icon" type="image/png" href="/images/icon_hue165b90e315bd195bed0e7c1c2e09531_14682_192x192_fill_lanczos_center_2.png">

  <link rel="canonical" href="/post/regularization/">

  
  
  
  
  
    
  
  
  <meta property="twitter:card" content="summary_large_image">
  
  <meta property="twitter:site" content="@keshav_vishal">
  <meta property="twitter:creator" content="@keshav_vishal">
  
  <meta property="og:site_name" content="Vishal Keshav">
  <meta property="og:url" content="/post/regularization/">
  <meta property="og:title" content="Priors and the relationship with regularization | Vishal Keshav">
  <meta property="og:description" content="In this article, we explore the relationship between regularization and the prior belief from baysian point of view."><meta property="og:image" content="/post/regularization/featured.jpg">
  <meta property="twitter:image" content="/post/regularization/featured.jpg"><meta property="og:locale" content="en-us">
  
    
      <meta property="article:published_time" content="2019-05-15T00:00:00&#43;00:00">
    
    <meta property="article:modified_time" content="2019-05-15T00:00:00&#43;00:00">
  

  


    






  




<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "/post/regularization/"
  },
  "headline": "Priors and the relationship with regularization",
  
  "image": [
    "/post/regularization/featured.jpg"
  ],
  
  "datePublished": "2019-05-15T00:00:00Z",
  "dateModified": "2019-05-15T00:00:00Z",
  
  "author": {
    "@type": "Person",
    "name": "Vishal Keshav"
  },
  
  "publisher": {
    "@type": "Organization",
    "name": "Vishal Keshav",
    "logo": {
      "@type": "ImageObject",
      "url": "/images/icon_hue165b90e315bd195bed0e7c1c2e09531_14682_192x192_fill_lanczos_center_2.png"
    }
  },
  "description": "In this article, we explore the relationship between regularization and the prior belief from baysian point of view."
}
</script>

  

  


  


  





  <title>Priors and the relationship with regularization | Vishal Keshav</title>

</head>

<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" >

  <aside class="search-results" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search">
        
      </div>

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>


  







<nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
  <div class="container">

    
    <div class="d-none d-lg-inline-flex">
      <a class="navbar-brand" href="/">Vishal Keshav</a>
    </div>
    

    
    <button type="button" class="navbar-toggler" data-toggle="collapse"
            data-target="#navbar-content" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
    <span><i class="fas fa-bars"></i></span>
    </button>
    

    
    <div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none">
      <a class="navbar-brand" href="/">Vishal Keshav</a>
    </div>
    

    
    
    <div class="navbar-collapse main-menu-item collapse justify-content-end" id="navbar-content">

      
      <ul class="navbar-nav d-md-inline-flex">
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#about"><span>Home</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link " href="/publication"><span>Research</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link " href="/projects"><span>Projects</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link  active" href="/post"><span>Blog Post</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#contact"><span>Contact</span></a>
        </li>

        
        

      

        
      </ul>
    </div>

    <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">
      
      <li class="nav-item">
        <a class="nav-link js-search" href="#"><i class="fas fa-search" aria-hidden="true"></i></a>
      </li>
      

      
      <li class="nav-item">
        <a class="nav-link js-dark-toggle" href="#"><i class="fas fa-moon" aria-hidden="true"></i></a>
      </li>
      

      

    </ul>

  </div>
</nav>


  <article class="article">

  




















  
  
    
  


<div class="article-container pt-3">
  <h1>Priors and the relationship with regularization</h1>

  
  <p class="page-subtitle">A closer look through bayesian statistics</p>
  

  


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span><a href="/authors/admin/">Vishal Keshav</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    May 15, 2019
  </span>
  

  

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    7 min read
  </span>
  

  
  
  

  
  

</div>

  














</div>


<div class="article-header article-container featured-image-wrapper mt-4 mb-4" style="max-width: 720px; max-height: 348px;">
  <div style="position: relative">
    <img src="/post/regularization/featured_hu164f5673c2404c6e59ea642ce52c618e_68853_720x0_resize_q90_lanczos.jpg" alt="" class="featured-image">
    <span class="article-header-caption">Image credit: <a href="https://unsplash.com/photos/CpkOjOcXdUY"><strong>Unsplash</strong></a></span>
  </div>
</div>



  <div class="article-container">

    <div class="article-style">
      <h1 id="introduction">Introduction</h1>
<p>Regularization techniques are a widely explored topic in machine learning that is used for improving generalization accuracy or reducing the overfitting of the data in a machine learning model. Regularization strategies involve either imposing a hard (or explicit) constraint or a soft (or implicit) constraints on the parameters of the model. Ridge regularizer (often referred to as $L2$) and Lasso regularizer (often referred to as $L1$) are some of the explicit regularizers that impose a direct constraint on the parameter. For example, $L1$ constraints the parameters in a space such that most of the parameter values equal to zero. Implicit regularization approach imposes a soft constraint on the parameters by modifying the optimization function. A popular approach called dropout is an example of such implicit regularizer that tunes the values of the parameters by the means of optimization function.</p>
<p>The goal of this article is neither to list down the regularization techniques commonly used in machine learning nor is to describe when and how the regularizers are being used, but to take a step back and understand the importance of prior (an expression of our belief on the values of the parameters) that is embodied in any regularization approach. Towards this end, we will look into two fundamental approaches of parameter estimation methods namely &ldquo;Maximum Likelihood Estimation (MLE)&rdquo; and &ldquo;Maximum a Posteriori (MAP)&rdquo; and see how the later approach regularizes the model better by using prior knowledge about the estimate. We will start with the first approach, MLE.</p>
<h1 id="mle-overfits">MLE Overfits</h1>
<p>Any meaningful data has some underlying statistical property. For any data we get, it is reasonable to assume that there exists a data generating distribution that has generated the subset of available data. Our objective is to infer the distribution by analyzing the hidden statistical property in the observed data.</p>
<p>Let us assume that the available data is given by $D$ where $D$ = $(x_{1},x_{2},&hellip;,x_{n})$ and each $x_{i}$ corresponds to $i^{th}$ data point and $x_{i}$ $\in$ $\mathbb{R}^{d}$. Furthermore, we assume that each data point is being generated independently by a common data generating distribution denoted by $P(X/\theta)$ or in other words, each data point $x_{i}$ is sampled from a random variable $X_{i}$ $\sim$  $P$.</p>
<p>We intend to estimate the true parameter $\theta$ of the assumed data distribution. MLE approaches this problem by maximizing the likelihood function $P(data/\theta)$, or more formally
$$
\begin{equation}
\theta_{MLE} = \operatorname*{arg,max}_\theta P(X_{1}=x_{1},X_{2}=x_{2},&hellip;,X_{n}=x_{n}/\theta)
\end{equation}
$$</p>
<p>Since it is assumed that the data distribution are identical and independent, by using the conditional independence rule (conditioned on $\theta$) on the joint probability distribution (over $X_{i}$), we get
$$
\begin{equation}
\theta_{MLE} = \operatorname*{arg,max}_\theta \prod_{i=1}^{n}P(X_{i}/\theta)
\end{equation}
$$</p>
<p>Taking a concrete example of data distribution, we can derive the maximum likelihood estimate. Lets suppose that data follows a Gaussian distribution (which is a reasonable assumption for scalar data points), and the parameters we are interested in is the mean (denoted by $\mu$) where we keep the variance (denoted by $\sigma^{2}$) a constant, i.e. $X_{i}$ $\sim$ $\mathcal{N}(\mu , \sigma^{2})$.
Here, the only parameter we want to estimate is the $\theta = \mu$.</p>
<p>Following the Gaussian density function for the given data distribution and the Independent and identically distributed (IID) assumptions, from previous equation we get
$$
\begin{equation}
\theta_{MLE} = \operatorname*{arg,max}_\theta \left ( \frac{1}{\sqrt{2\pi\sigma^{{2}}}} \right ) exp\left ( -\frac{1}{2\sigma^{2}}\sum_{i=1}^{n}(x_{i} - \mu)^2 \right )
\end{equation}
$$</p>
<p>Since the logarithm is a monotonically increasing function, we can maximize $\log$ of the right side expression and equivalently get the parameter of interest.</p>
<p>$$
\begin{align}
\theta_{MLE} &amp; = \operatorname*{arg,max}_\theta \log P(data/\theta) \\\<br>
&amp; = \operatorname*{arg,max}_\theta -\frac{n}{2}\log(2\pi\sigma^2) - \frac{1}{2\sigma^2}\sum_{i=1}^{n}(x_{i} - \mu)
\end{align}
$$</p>
<p>fetches us $\mu$ = $\frac{\sum_{i=1}^{n}x_{i}}{n}$ which is the sample mean. Here we can see that if the dataset is small, the MLE estimate memorizes both the signal and the noise from the data even if we may have some idea of where the parameter should lie.</p>
<p>An apt representation of this is idea is shown in the figure below.</p>
<p><img src="res/figure.png" alt="figure"></p>
<p>When the number of parameter increases and the number of data point remains small, the MLE estimates a complicated function that more often tries to fit the noise present in the data and diverges from the actual data distribution. In the next section, we will see how the MAP estimate overcome this drawback by introducing something called &ldquo;prior&rdquo;.</p>
<h1 id="map-regularizes">MAP Regularizes</h1>
<p>Our problem statement remains the same as that of MLE, but our assumptions about the underlying statistical properties of the data is a bit different. We now assume that we have a full knowledge about the joint distribution of data and the underlying data distribution&rsquo;s parameter, as denoted by $P(X_{1}, X_{2},&hellip;,X_{n},\theta)$. Here, we note that the $\theta$ is now a random variable. Although $\theta$ is a random variable now, MAP estimates are the point estimates that evaluate the parameter that maximizes a certain condition, in this case it is the posterior distribution of $\theta$ given the data. Formally,</p>
<p>$$
\begin{align}
\theta_{MAP} &amp; = \operatorname*{arg,max}_\theta P(\theta/X_{1},X_{2},&hellip;,X_{n})\\\<br>
&amp; = \operatorname*{arg,max}_\theta \frac{P(X_{1}, X_{2},&hellip;,X_{n},\theta)}{P(X_{1}, X_{2},&hellip;,X_{n})}\\\<br>
&amp; = \operatorname*{arg,max}_\theta P(X_{1}, X_{2},&hellip;,X_{n}/\theta)*P(\theta)
\end{align}
$$</p>
<p>using proportionality and Bayes theorem.
Taking log and using IID assumption on right-hand side of the expression, we get
$$
\begin{equation}
\theta_{MAP} = \operatorname*{arg,max}_\theta \sum_{i=1}^{n}\log P(X_{i} = x_{i}/\theta) + \log P(\theta)
\end{equation}
$$</p>
<p>We again take a concrete example by assuming a joint distribution of data and parameter. Alternatively, we assume that the conditional data distribution conditioned on parameter $\mu$ follows $\mathcal{N}(\mu, \sigma^{2})$ where $\theta = \mu$, $\sigma$ is kept as a constant and the parameter $\mu$ is distributed normally with mean $\mu^{*}$ and a variance of 1 i.e. $\mu$ = $\theta$ $\sim$ $\mathcal{N}(\mu^{*}, 1)$. Under these univariate Gaussian assumptions for data and parameter, we can maximize the logarithm of the posterior to estimate the point estimate for $\theta$.
$$
\begin{align}
\theta_{MAP} = \operatorname*{arg,max}_\theta &amp; \left ( \frac{1}{\sqrt{2\pi\sigma^{{2}}}} \right ) exp\left ( -\frac{1}{2\sigma^{2}}\sum_{i=1}^{n}(x_{i} - \mu)^2 \right ) + \\\ &amp; \left ( \frac{1}{\sqrt{2\pi}} \right ) exp\left ( -\frac{1}{2}(\mu - \mu^{*})^2 \right )
\end{align}
$$</p>
<p>Differentiating the right-hand side and equating it to zero, the expression evaluates to
$$
\begin{equation}
\frac{1}{\sigma^{2}}\left ( \sum_{i=1}^{n}(x_{i}-n\mu)^{2} \right ) + (\mu^{*} - \mu) = 0
\end{equation}
$$
Rearranging the $\mu$ on one side, we get
$\mu = \frac{\sum_{i=1}^{n}x_{i}}{\sigma^2 + n} + \frac{\sigma^{2}}{\sigma^{2}+n}\mu^{*}$
Since, $\overline{x} = \frac{\sum_{i=1}^{n}x_{i}}{n}$ is the sample mean, we can write the above equation as
$$
\begin{equation}
\theta_{MAP} = \frac{n}{\sigma^2 + n}\overline{x} + \frac{\sigma^{2}}{\sigma^{2}+n}\mu^{*}
\end{equation}
$$
From the above expression, we can see that the $\theta_{MLE}$ is a convex combination of sample mean $\overline{x}$ and the prior mean $\mu^{*}$. The prior mean $\mu^{*}$ reflects our prior belief about where the mean should be before we observe any data. When the number of data points are less, more weightage is given on the prior mean which helps in achieving regularization through reducing the data overfitting. In an extream case where $n \rightarrow \infty$, $\frac{n}{\sigma^2 + n} \rightarrow 1$ and $\frac{\sigma^{2}}{\sigma^{2}+n} \rightarrow 0 $, the parameter estimation is dominated by sample mean (which will reasonably estimate the actual parameter of the data distribution). When the number of data points are sufficient, the parameter estimate is well guided by the observed data, otherwise we partially rely on our prior knowledge of what value a parameter should assume.</p>
<p>Refer the figure below for an intuitive understanding.
<img src="res/figure2.png" alt="figure2"></p>
<h1 id="summary">Summary</h1>
<p>In this article, we explored two fundamental parameter estimation algorithm namely Maximum Likelihood Estimation and Maximum a Posteriori. By taking concrete examples of the distribution (along with several reasonable assumptions), we argued that MAP estimates induce regularization through prior over the parameters.</p>
<p>In a traditional Bayesian setup, a prior is explicitly modeled, but in a deep learning setting, this may take several forms. Whatever be the case, prior knowledge induces better inductive bias and this can be achieved through designing the architecture, curating of the training data and the choosing a better optimization objective. A more direct approach of imposing the prior for regularizing a model can be seen in the process of distillation. A teacher network is trained without any prior data, and when it is trained, the knowledge (in terms of prior) is distilled in the student network. Formulating a better and more explicit approach to induce prior in deep learning systems is an active area of research which we will explore further in subsequent articles.</p>

    </div>

    





<div class="article-tags">
  
  <a class="badge badge-light" href="/tags/academic/">Academic</a>
  
</div>



<div class="share-box" aria-hidden="true">
  <ul class="share">
    
      
      
      
        
      
      
      
      <li>
        <a href="https://twitter.com/intent/tweet?url=/post/regularization/&amp;text=Priors%20and%20the%20relationship%20with%20regularization" target="_blank" rel="noopener" class="share-btn-twitter">
          <i class="fab fa-twitter"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.facebook.com/sharer.php?u=/post/regularization/&amp;t=Priors%20and%20the%20relationship%20with%20regularization" target="_blank" rel="noopener" class="share-btn-facebook">
          <i class="fab fa-facebook"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="mailto:?subject=Priors%20and%20the%20relationship%20with%20regularization&amp;body=/post/regularization/" target="_blank" rel="noopener" class="share-btn-email">
          <i class="fas fa-envelope"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.linkedin.com/shareArticle?url=/post/regularization/&amp;title=Priors%20and%20the%20relationship%20with%20regularization" target="_blank" rel="noopener" class="share-btn-linkedin">
          <i class="fab fa-linkedin-in"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://web.whatsapp.com/send?text=Priors%20and%20the%20relationship%20with%20regularization%20/post/regularization/" target="_blank" rel="noopener" class="share-btn-whatsapp">
          <i class="fab fa-whatsapp"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://service.weibo.com/share/share.php?url=/post/regularization/&amp;title=Priors%20and%20the%20relationship%20with%20regularization" target="_blank" rel="noopener" class="share-btn-weibo">
          <i class="fab fa-weibo"></i>
        </a>
      </li>
    
  </ul>
</div>












  
    
    





  
  
  
    
  
  
  
  <div class="media author-card content-widget-hr">
    
      
      <img class="avatar mr-3 avatar-circle" src="/authors/admin/avatar_hu0db5ae1dc816e37943b7f48026c9322f_205454_270x270_fill_lanczos_center_2.png" alt="Avatar">
    

    <div class="media-body">
      <h5 class="card-title"><a href="/">Vishal Keshav</a></h5>
      <h6 class="card-subtitle">Computer Science Graduate</h6>
      <p class="card-text">CS graduate @ UMass</p>
      <ul class="network-icon" aria-hidden="true">
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="/#contact" >
        <i class="fas fa-envelope"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://twitter.com/keshav_vishal" target="_blank" rel="noopener">
        <i class="fab fa-twitter"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://www.linkedin.com/in/vkeshav" target="_blank" rel="noopener">
        <i class="fab fa-linkedin"></i>
      </a>
    </li>
  
    
    
    
    
    
    
    
      
    
    <li>
      <a href="https://scholar.google.co.in/citations?user=YOQjzP4AAAAJ&amp;hl=en" target="_blank" rel="noopener">
        <i class="ai ai-google-scholar"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://github.com/vishal-keshav" target="_blank" rel="noopener">
        <i class="fab fa-github"></i>
      </a>
    </li>
  
    
    
    
    
    
    
    
      
    
    <li>
      <a href="https://orcid.org/0000-0003-3912-7589" target="_blank" rel="noopener">
        <i class="ai ai-orcid"></i>
      </a>
    </li>
  
</ul>

    </div>
  </div>


  








<div class="article-widget">
  
<div class="post-nav">
  
  
  
  <div class="post-nav-item">
    <div class="meta-nav">Next</div>
    <a href="/post/python-variables/" rel="next">Python is a funny language!</a>
  </div>
  
  
  
  <div class="post-nav-item">
    <div class="meta-nav">Previous</div>
    <a href="/post/review-cv/" rel="prev">A review of Computer Vision Architecture</a>
  </div>
  
</div>

</div>



  
  
  <div class="article-widget content-widget-hr">
    <h3>Related</h3>
    <ul>
      
      <li><a href="/post/review-cv/">A review of Computer Vision Architecture</a></li>
      
    </ul>
  </div>
  



  </div>
</article>

      

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js" integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin="anonymous"></script>

      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mermaid/8.4.8/mermaid.min.js" integrity="sha256-lyWCDMnMeZiXRi7Zl54sZGKYmgQs4izcT7+tKc+KUBk=" crossorigin="anonymous" title="mermaid"></script>
      

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js" integrity="sha256-eOgo0OtLL4cdq7RdwRUiGKLX9XsIJ7nGhWEKbohmVAQ=" crossorigin="anonymous"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/languages/r.min.js"></script>
        
      

    

    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.5.1/leaflet.js" integrity="sha256-EErZamuLefUnbMBQbsEqu1USa+btR2oIlCpBJbyD4/g=" crossorigin="anonymous"></script>
    

    
    
    <script>const code_highlighting = true;</script>
    

    
    
    <script>const isSiteThemeDark = false;</script>
    

    
    
    
    
    
    
    <script>
      const search_config = {"indexURI":"/index.json","minLength":1,"threshold":0.3};
      const i18n = {"no_results":"No results found","placeholder":"Search...","results":"results found"};
      const content_type = {
        'post': "Posts",
        'project': "Projects",
        'publication' : "Publications",
        'talk' : "Talks"
        };
    </script>
    

    
    

    
    
    <script id="search-hit-fuse-template" type="text/x-template">
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script>
    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
    

    
    

    
    

    
    
    
    
    
    
    
    
    
      
    
    
    
    
    <script src="/js/academic.min.c816d323c3a55093dae0829b44ea1ca8.js"></script>

    






  
  
  <div class="container">
    <footer class="site-footer">

  <p class="powered-by">
    

    Â© 2020 Vishal Keshav. All thoughts and opinions here are my own. 

    
    <span class="float-right" aria-hidden="true">
      <a href="#" class="back-to-top">
        <span class="button_icon">
          <i class="fas fa-chevron-up fa-2x"></i>
        </span>
      </a>
    </span>
    
  </p>
</footer>

  </div>
  

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

</body>
</html>
